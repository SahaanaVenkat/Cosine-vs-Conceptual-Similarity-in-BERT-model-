{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\nlp_workshop\\amazon-reviews\\amazon\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_recommendation = [\n",
    "    {\"text\": \"how to maintain health\", \"label\": 0},\n",
    "    {\"text\": \"how to protect health\", \"label\": 1},\n",
    "    {\"text\": \"factors affecting health\", \"label\": 2},\n",
    "    {\"text\": \"bad factors for health\", \"label\": 3},\n",
    "    {\"text\": \"good factors for health\", \"label\": 4},\n",
    "    {\"text\": \"preventing shutdown in health\", \"label\": 5},\n",
    "    {\"text\": \"optimizing health performance\", \"label\": 6},\n",
    "    {\"text\": \"extending lifespan in health\", \"label\": 7},\n",
    "    {\"text\": \"enhancing health efficiency\", \"label\": 8},\n",
    "    {\"text\": \"key parameters to monitor in health\", \"label\": 9}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Conceptual similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained BERT model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "df = pd.DataFrame(data_for_recommendation)\n",
    "\n",
    "# Embed the texts\n",
    "embeddings = model.encode(df['text'], convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate similarity\n",
    "def calculate_similarity(query, embeddings):\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    cos_scores = util.pytorch_cos_sim(query_embedding, embeddings)[0]\n",
    "    return cos_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Find similar texts for a given query\n",
    "def input_text_model(input_text):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        input_text (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    \n",
    "    input_sentence_processed = ''.join([i for i in input_text if not i.isdigit()])\n",
    "    query = input_sentence_processed\n",
    "    query_similarity = calculate_similarity(query, embeddings)\n",
    "    top_n = 10\n",
    "# Convert PyTorch tensor to a NumPy array and move to CPU if necessary\n",
    "    top_indices = query_similarity.argsort().cpu().numpy()[-top_n:]\n",
    "    similar_texts = df.loc[top_indices, 'label'].tolist()\n",
    "    label=similar_texts[-1]\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label=input_text_model(\"Tips for preserving health?\")\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load BERT model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize and encode the dataset for recommendation function only\n",
    "tokenized_data_recommendation = tokenizer(\n",
    "    [item[\"text\"] for item in data_for_recommendation],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "def input_text_model(input_text):\n",
    "    \"\"\"Predict label for input text\"\"\"\n",
    "    # Process input text\n",
    "      \n",
    "    # Tokenize the input sentence\n",
    "    tokenized_input = tokenizer(\n",
    "        input_text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Get the embeddings for the input sentence\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**tokenized_input)\n",
    "        input_embeddings = model_output.logits.detach().numpy()\n",
    "    \n",
    "    # Get the embeddings for sentences in both datasets\n",
    "    embeddings_recommendation = model(**tokenized_data_recommendation).logits.detach().numpy()\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity_recommendation = cosine_similarity(input_embeddings, embeddings_recommendation)\n",
    "\n",
    "    # Determine the label based on higher cosine similarity\n",
    "    most_similar_index_recommendation = np.argmax(similarity_recommendation)\n",
    "    label_recommendation = data_for_recommendation[most_similar_index_recommendation][\"label\"]\n",
    "    return label_recommendation\n",
    "\n",
    "# Sample usage\n",
    "\n",
    "label = input_text_model(\"what is the good kiln temperature?\")\n",
    "print(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The two code snippets you provided perform similar tasks of predicting labels or categories based on input text. However, they use different models and libraries for text embedding and similarity calculation.\n",
    "\n",
    "First Code Snippet:\n",
    "\n",
    "Uses the BERT model (BertForSequenceClassification) from the Hugging Face Transformers library.\n",
    "Tokenizes the input text and encodes it using the BERT tokenizer.\n",
    "Calculates the embeddings for both the input text and the texts in the dataset using the BERT model.\n",
    "Measures similarity between the input embeddings and the dataset embeddings using cosine similarity.\n",
    "Determines the label or category based on the highest cosine similarity score.\n",
    "Relies on the cosine_similarity function from scikit-learn for similarity calculation.\n",
    "Second Code Snippet:\n",
    "\n",
    "Uses the Sentence Transformer library.\n",
    "Loads a pre-trained Sentence Transformer model (paraphrase-MiniLM-L6-v2).\n",
    "Embeds both the input text and the texts in the dataset using the Sentence Transformer model.\n",
    "Calculates similarity scores between the input embedding and the dataset embeddings using cosine similarity.\n",
    "Determines the label or category based on the most similar texts.\n",
    "Uses functions provided by the Sentence Transformer library (encode and pytorch_cos_sim) for embedding and similarity calculation.\n",
    "In summary, both snippets achieve similar functionality but use different libraries and models for text embedding and similarity calculation. The choice between them depends on factors such as model preference, performance requirements, and ease of integration with existing codebases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
